{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import itertools\n",
    "import csv\n",
    "import sys\n",
    "from typing import Any, Callable, Dict, List\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a904e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory (../) to sys.path\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "from utils.metrics import compute_sensitivity_false_alarm_rate_timing_tolerance\n",
    "from matrix_profile import MatrixProfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4d9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files_with_prefix(directory: str, prefix: str) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Finds all files in the given directory that start with the specified prefix.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory to search.\n",
    "        prefix (str): The prefix that filenames should start with.\n",
    "\n",
    "    Returns:\n",
    "        List[Path]: List of Path objects for matching files.\n",
    "    \"\"\"\n",
    "    dir_path = Path(directory)\n",
    "    return [file for file in dir_path.iterdir() if file.is_file() and file.name.startswith(prefix)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed748ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount_of_annomalies_per_record = 250\n",
    "# amount_of_records = 3500\n",
    "# batch_size_load = 100\n",
    "# downsample_freq = 8\n",
    "# max_gap_annos_in_sec = 2\n",
    "# n_cons = 5 # annomalies\n",
    "# window_size_sec = 25\n",
    "# pre_thresh_sec = 60*5\n",
    "# post_thresh_sec = 60*3\n",
    "# DIR_preprocessed =  f\"/home/swolf/asim_shared/preprocessed_data/downsample_freq={downsample_freq},no_windows\"\n",
    "# MPs_path = f\"/home/swolf/asim_shared/results/MP/downsample_freq={downsample_freq},no_windows/seq_len{window_size_sec}sec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6969a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "613b7a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mps_filenames = [filename for filename in os.listdir(MPs_path) if filename.endswith(\".pkl\")]\n",
    "# len(mps_filenames)*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9ce4cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_mp_results(amount_of_annomalies_per_record: int, amount_of_records:int, batch_size_load :int, downsample_freq:int, max_gap_annos_in_sec:int, n_cons:int, window_size_sec:int, pre_thresh_sec:int, post_thresh_sec:int, DIR_preprocessed:str, MPs_path:str, verbose:bool):\n",
    "    mps_filenames = [filename for filename in os.listdir(MPs_path) if filename.endswith(\".pkl\")]\n",
    "    # preprocessed_filenames = [filename for filename in os.listdir(DIR_preprocessed) if filename.endswith(\"_preprocessed.pkl\")]\n",
    "    mps_list = []\n",
    "    label_list = []\n",
    "    tp_list = []\n",
    "    fp_list = []\n",
    "    hours_list = []\n",
    "    total_events_list = []\n",
    "    for i, mp_filename in enumerate(mps_filenames[:amount_of_records], 1):\n",
    "        mp_filename=mp_filename.__str__()\n",
    "        # Load MP\n",
    "        with open(os.path.join(MPs_path, mp_filename), \"rb\") as f:\n",
    "            mps_list.append(pickle.load(f)[:, 0].reshape(-1, 1))\n",
    "        with open(os.path.join(DIR_preprocessed, mp_filename[3:-4]+\"_preprocessed.pkl\"), \"rb\") as g:\n",
    "            label_list.append(pickle.load(g)[\"channels\"][0][\"labels\"][0])\n",
    "        if i%batch_size_load ==0:\n",
    "            if verbose:\n",
    "                print(i)\n",
    "            if not len(label_list)==len(mps_list):\n",
    "                print(f\"len(label_list) not ==len(mps_list)\")\n",
    "            annomaly_indices = [MatrixProfile.get_top_k_anomaly_indices(matrix_profile=mp.flatten(), k=amount_of_annomalies_per_record)\n",
    "                        for mp in mps_list]\n",
    "            annomaly_indices_cons = [MatrixProfile.mean_of_all_consecutive_anomalies(indices=annos, n=n_cons, max_gap = downsample_freq*max_gap_annos_in_sec)\n",
    "                            for annos in annomaly_indices]\n",
    "            true_positives, false_positives, hours, total_events = compute_sensitivity_false_alarm_rate_timing_tolerance(\n",
    "                label_sequences=label_list, detection_indices=annomaly_indices_cons, lower=pre_thresh_sec, upper=post_thresh_sec, frequency=downsample_freq\n",
    "                )\n",
    "            tp_list.append(true_positives)\n",
    "            fp_list.append(false_positives)\n",
    "            hours_list.append(hours)\n",
    "            total_events_list.append(total_events)\n",
    "            mps_list = []\n",
    "            label_list = []\n",
    "    if sum(total_events_list) > 0:\n",
    "        sensitivity = sum(tp_list)/sum(total_events_list)\n",
    "    else:\n",
    "        sensitivity = 0.0\n",
    "\n",
    "    if sum(hours_list) > 0:\n",
    "        false_alarms_per_hour = sum(fp_list)/sum(hours_list)\n",
    "    else:\n",
    "        false_alarms_per_hour = 0.0\n",
    "    # false_alarms_per_hour = sum(fp_list)/sum(hours_list) if sum(hours_list) > 0 else 0.0\n",
    "    overview = {\"# TP\":sum(tp_list), \"# FP\": sum(fp_list), \"# Total seizures\":sum(total_events_list)}\n",
    "    return sensitivity, false_alarms_per_hour, overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46bd8546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination: {'amount_of_annomalies_per_record': 125, 'amount_of_records': 59, 'batch_size_load': 100, 'downsample_freq': 8, 'max_gap_annos_in_sec': 2, 'n_cons': 5, 'window_size_sec': 25, 'pre_thresh_sec': 300, 'post_thresh_sec': 180, 'verbose': True, 'DIR_preprocessed': '/home/swolf/asim_shared/preprocessed_data/downsample_freq=8,no_windows', 'MPs_path': '/home/swolf/asim_shared/results/MP/downsample_freq=8,no_windows/seq_len25sec'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2318829/2045627694.py:16: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  label_list.append(pickle.load(g)[\"channels\"][0][\"labels\"][0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination: {'amount_of_annomalies_per_record': 250, 'amount_of_records': 59, 'batch_size_load': 100, 'downsample_freq': 8, 'max_gap_annos_in_sec': 2, 'n_cons': 5, 'window_size_sec': 25, 'pre_thresh_sec': 300, 'post_thresh_sec': 180, 'verbose': True, 'DIR_preprocessed': '/home/swolf/asim_shared/preprocessed_data/downsample_freq=8,no_windows', 'MPs_path': '/home/swolf/asim_shared/results/MP/downsample_freq=8,no_windows/seq_len25sec'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2318829/2045627694.py:16: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  label_list.append(pickle.load(g)[\"channels\"][0][\"labels\"][0])\n"
     ]
    }
   ],
   "source": [
    "def run_grid_search(param_grid: Dict[str, List[Any]],\n",
    "                    target_function: Callable[..., Dict[str, Any]],\n",
    "                    save_results: bool = False) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Executes a grid search over all combinations of parameter values and optionally saves results to CSV.\n",
    "\n",
    "    Args:\n",
    "        param_grid (Dict[str, List[Any]]): Parameter grid with possible values for each parameter.\n",
    "        target_function (Callable): The function to evaluate.\n",
    "        save_results (bool): Whether to save results to CSV.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of parameter combinations and their results.\n",
    "    \"\"\"\n",
    "    # csv_path = \"/home/jhagenbe_sw/ASIM/ecg-seizure-detection/MatrixProfile/hp_tuning_mp_results.csv\"\n",
    "    excel_path = \"/home/jhagenbe_sw/ASIM/ecg-seizure-detection/MatrixProfile/hp_tuning_mp_results.xlsx\"\n",
    "\n",
    "    # os.makedirs(os.path.dirname(excel_path), exist_ok=True)\n",
    "    # file_exists = os.path.isfile(excel_path)\n",
    "\n",
    "    keys = list(param_grid.keys())\n",
    "    combinations = list(itertools.product(*(param_grid[key] for key in keys)))\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for values in combinations:\n",
    "        params = dict(zip(keys, values))\n",
    "        print(f\"Testing combination: {params}\")\n",
    "        try:\n",
    "            # result = target_function(**params)\n",
    "            sensitivity, false_alarms_per_hour, overview = target_function(**params)\n",
    "            combined = {\n",
    "                **params,\n",
    "                \"sensitivity\": sensitivity,\n",
    "                \"false_alarms_per_hour\": false_alarms_per_hour,\n",
    "                \"overview\": overview\n",
    "            }\n",
    "\n",
    "            results.append(combined)\n",
    "\n",
    "            # if save_results:\n",
    "            #     write_mode = 'a' if file_exists else 'w'\n",
    "            #     with open(csv_path, write_mode, newline='') as csvfile:\n",
    "            #         writer = csv.DictWriter(csvfile, fieldnames=list(combined.keys()))\n",
    "            #         if not file_exists:\n",
    "            #             writer.writeheader()\n",
    "            #             file_exists = True\n",
    "            #         writer.writerow(combined)\n",
    "            if save_results:\n",
    "                df_row = pd.DataFrame([combined])\n",
    "\n",
    "                if os.path.isfile(excel_path):\n",
    "                    # Load existing Excel file and append new row\n",
    "                    existing_df = pd.read_excel(excel_path)\n",
    "                    df_combined = pd.concat([existing_df, df_row], ignore_index=True)\n",
    "                else:\n",
    "                    df_combined = df_row\n",
    "                # Write the full DataFrame back to the file\n",
    "                df_combined.to_excel(excel_path, index=False)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with parameters {params}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return results\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    # Example parameter grid\n",
    "\n",
    "downsample_freq = 8\n",
    "window_size_sec = 25\n",
    "parameter_grid: Dict[str, List[Any]] = {\n",
    "    \"amount_of_annomalies_per_record\": [125, 250],\n",
    "    \"amount_of_records\": [59], # 2795 * 0.2 => 20% of samples\n",
    "    \"batch_size_load\": [100],\n",
    "    \"downsample_freq\": [downsample_freq],\n",
    "    \"max_gap_annos_in_sec\": [2],\n",
    "    \"n_cons\": [5],\n",
    "    \"window_size_sec\": [window_size_sec],\n",
    "    \"pre_thresh_sec\": [60 * 5],\n",
    "    \"post_thresh_sec\": [60 * 3],\n",
    "    \"verbose\": [True],\n",
    "    \"DIR_preprocessed\": [f\"/home/swolf/asim_shared/preprocessed_data/downsample_freq={downsample_freq},no_windows\"],\n",
    "    \"MPs_path\": [f\"/home/swolf/asim_shared/results/MP/downsample_freq={downsample_freq},no_windows/seq_len{window_size_sec}sec\"]\n",
    "}\n",
    "\n",
    "    # Run grid search with saving enabled\n",
    "grid_search_results = run_grid_search(parameter_grid, produce_mp_results, save_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "780312e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0f5c103",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tp_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msensitivity= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(\u001b[43mtp_list\u001b[49m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28msum\u001b[39m(total_events_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalse alarms per hour \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(fp_list)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28msum\u001b[39m(hours_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(tp_list)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(fp_list)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(total_events_list)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tp_list' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"sensitivity= {sum(tp_list)/sum(total_events_list)}\")\n",
    "print(f\"False alarms per hour {sum(fp_list)/sum(hours_list)}\")\n",
    "print(f\"{sum(tp_list)=}, {sum(fp_list)=}, {sum(total_events_list)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b90d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'# TP': 300, '# FP': 19680, '# Total seizures': 832}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"# TP\":sum(tp_list),\n",
    " \"# FP\": sum(fp_list),\n",
    " \"# Total seizures\":sum(total_events_list)}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ced60653",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "sensitivity= 0.37410071942446044\n",
    "False alarms per hour 1.7258375827112402\n",
    "sum(tp_list)=52, sum(fp_list)=3711, sum(total_events_list)=139"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASIM_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
